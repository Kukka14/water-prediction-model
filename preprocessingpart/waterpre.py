# -*- coding: utf-8 -*-
"""WaterPre.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lF2k-w8YCiFF_l5g1jFP1u9OsbgLlRvN
"""

import pandas as pd

dataset = pd.read_csv('/content/Reduced_Water_Quality_Prediction.csv')

dataset.shape

# Check for typos and ensure column names exist
print(dataset.columns)

# Drop columns (adjust column names if needed based on the output above)
dataset = dataset.drop(dataset.columns[0], axis=1)

print(dataset['Target'].value_counts())

dataset.info()

dataset.isnull().sum()

dataset.isnull().sum()/dataset.shape[0]*100

#finding duplicates
dataset.duplicated().sum()

dataset.drop_duplicates(inplace=True)
dataset.duplicated().sum()

dataset.shape

#garbage values
for i in dataset.select_dtypes(include="object").columns:
  print(dataset[i].value_counts())
  print("***"*10)

#descriptive statistics
dataset.describe().T

dataset.describe(include="object")

import seaborn as sns
import matplotlib.pyplot as plt
for i in dataset.select_dtypes(include="number").columns:
    sns.boxplot(data=dataset,x=i)
    plt.show()

dataset.isnull().sum()  #checking for missing values

#add mean to missing value (No outliers or less outliers)
dataset['Odor'] = dataset['Odor'].fillna(dataset['Odor'].mean())

dataset['Total Dissolved Solids'] = dataset['Total Dissolved Solids'].fillna(dataset['Total Dissolved Solids'].mean())

dataset['Day'] = dataset['Day'].fillna(dataset['Day'].mean())

dataset['Time of Day'] = dataset['Time of Day'].fillna(dataset['Time of Day'].mean())

dataset.isnull().sum()

for column in dataset.select_dtypes(include='number').columns:
  if column not in ['Odor', 'Total Dissolved Solids', 'Day', 'Time of Day']:
    dataset[column] = dataset[column].fillna(dataset[column].median())

dataset.isnull().sum()

dataset.isnull().sum()

for column in dataset.select_dtypes(include='object').columns:
  dataset[column] = dataset[column].fillna(dataset[column].mode()[0])

dataset.isnull().sum()

dataset.to_csv('new_dataset.csv', index=False)

from google.colab import files
files.download('new_dataset.csv')

for i in dataset.select_dtypes(include="number").columns:
    sns.boxplot(data=dataset,x=i)
    plt.show()

dataset.isnull().sum()

# Check for typos and ensure column names exist
print(dataset.columns)

# Drop columns (adjust column names if needed based on the output above)
dataset = dataset.drop(['Iron', 'Lead', 'Manganese'], axis=1, errors='ignore') # Use errors='ignore' to skip non-existent columns

dataset.shape

from scipy.stats.mstats import winsorize
import numpy as np

numerical_cols = dataset.select_dtypes(include=np.number).columns.tolist()
for col in numerical_cols:
  if col not in ['Iron', 'Lead', 'Manganese', 'Target']:
    dataset[col] = winsorize(dataset[col], limits=[0.05, 0.05])

dataset.shape

for i in dataset.select_dtypes(include="number").columns:
    sns.boxplot(data=dataset,x=i)
    plt.show()

!pip install scikit-learn

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Assuming 'dataset' is your DataFrame
for column in dataset.select_dtypes(include='object').columns:
    le = LabelEncoder()
    dataset[column] = le.fit_transform(dataset[column])

dataset.to_csv('encoded_dataset.csv', index=False) # Save the DataFrame to a CSV file named 'encoded_dataset.csv'

!ls -l encoded_dataset.csv # List the file in the current directory to verify it exists

from google.colab import files # imports the files module from google.colab
files.download('encoded_dataset.csv') # downloads the file named 'encoded_dataset.csv'

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE


# Initialize the model
rf = RandomForestClassifier()

# RFE for feature ranking
rfe = RFE(estimator=rf, n_features_to_select=10)
rfe = rfe.fit(dataset, dataset['Target']) # Changed 'data' to 'dataset'

# Get selected features
selected_features = dataset.columns[rfe.support_]

# Update dataset with selected features
dataset = dataset[selected_features] # Changed 'data' to 'dataset'

# Fit Random Forest to the data
rf.fit(dataset, dataset['Target'])

# Get feature importances
feature_importances = rf.feature_importances_

# Sort and display important features
sorted_idx = np.argsort(feature_importances)[::-1]
for index in sorted_idx:
    print(f"Feature: {dataset.columns[index]}, Importance: {feature_importances[index]}")

!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

# Separate features and target variable
X = dataset.drop('Target', axis=1)
y = dataset['Target']

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new balanced dataset
balanced_dataset = pd.concat([X_resampled, y_resampled], axis=1)

print(balanced_dataset['Target'].value_counts())

balanced_dataset.shape[0]

balanced_dataset.isnull().sum()

balanced_dataset.duplicated().sum()

balanced_dataset.drop_duplicates(inplace=True)

balanced_dataset.duplicated().sum()

from google.colab import files

# Save the DataFrame to a csv file
balanced_dataset.to_csv('balanced_dataset.csv', index=False)

files.download('balanced_dataset.csv')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    balanced_dataset.drop('Target', axis=1), balanced_dataset['Target'], test_size=0.3, random_state=42
)

# Initialize Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf.predict(X_test)

print(f"Training set shape: {X_train.shape}") # Print the shape of the training data (rows, columns)
print(f"Test set shape: {X_test.shape}") # Print the shape of the test data (rows, columns)

from sklearn.metrics import accuracy_score, classification_report

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate model
print("Accuracy: ", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

